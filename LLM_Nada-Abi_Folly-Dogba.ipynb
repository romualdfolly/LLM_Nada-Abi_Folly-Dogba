{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6404de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time\n",
    "import re, collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "315de967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilisé : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device utilisé : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bd46d",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4193f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    \"\"\" This class is used to tokenize the input text and create a vocabulary of tokens. \"\"\"\n",
    "    def __init__(self, text, num_merges):\n",
    "        self.text = text\n",
    "        self.num_merges = num_merges\n",
    "\n",
    "    def initialize_vocabulary(self, text):\n",
    "        \"\"\" Initialize the vocabulary from the input text. \"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        words = text.strip().split()\n",
    "        for word in words:\n",
    "            vocab[' '.join(list(word)) + ' '] += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_tokens_and_frequencies(self, vocab):\n",
    "        \"\"\" Get the tokens and how often they occur in the vocabulary. \"\"\"\n",
    "        tokens = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            word_tokens = word.split()\n",
    "            for token in word_tokens:\n",
    "                tokens[token] += freq\n",
    "        return tokens\n",
    "    \n",
    "    def get_pairs_and_counts(self, vocab):\n",
    "        \"\"\" Get the pairs of tokens and how often they occur in the vocabulary. \"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[symbols[i],symbols[i+1]] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_pair_in_vocabulary(self, pair, vocab_in):\n",
    "        \"\"\" Merge the most frequent pair of tokens in the vocabulary. \"\"\"\n",
    "        vocab_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in vocab_in:\n",
    "            word_out = p.sub(''.join(pair), word)\n",
    "            vocab_out[word_out] = vocab_in[word]\n",
    "        return vocab_out\n",
    "    \n",
    "    def tokenize(self):\n",
    "        \"\"\" Tokenize the input text. \"\"\"\n",
    "        if os.path.exists('./data/tokens_{}merges.pt'.format(self.num_merges)):\n",
    "            print(\"Loading tokenized data from file\")\n",
    "            tokens = torch.load('./data/tokens_{}merges.pt'.format(self.num_merges),weights_only=False)\n",
    "            vocab = torch.load('./data/vocab_{}merges.pt'.format(self.num_merges),weights_only=False)\n",
    "            return tokens, vocab\n",
    "        \n",
    "        # Initialize the vocabulary from the input text\n",
    "        vocab = self.initialize_vocabulary(self.text)\n",
    "\n",
    "        # Merge the most frequent pair of tokens num_merges times\n",
    "        with tqdm.tqdm(range(self.num_merges), position=0, leave=True) as pbar:\n",
    "            for i in pbar:\n",
    "                tokens = self.get_tokens_and_frequencies(vocab)\n",
    "                pairs = self.get_pairs_and_counts(vocab)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                most_frequent_pair = max(pairs, key=pairs.get)\n",
    "                vocab = self.merge_pair_in_vocabulary(most_frequent_pair, vocab)\n",
    "                pbar.set_description(f'Num merges: {i + 1}')\n",
    "\n",
    "        # Find the tokens and how often they occur in the vocabulary one last time\n",
    "        tokens = self.get_tokens_and_frequencies(vocab)\n",
    "\n",
    "        tokens[' '] = self.text.count(' ')\n",
    "        tokens['\\n'] = self.text.count('\\n')\n",
    "\n",
    "        torch.save(tokens, './data/tokens_{}merges.pt'.format(self.num_merges))\n",
    "        torch.save(vocab, './data/vocab_{}merges.pt'.format(self.num_merges))\n",
    "\n",
    "        return tokens, vocab\n",
    "\n",
    "\n",
    "    def tokenize_text(self, tokens):\n",
    "        \"\"\" Tokenize the input text using the tokens. \"\"\"\n",
    "        # Load tokenized data if it exists\n",
    "        if os.path.exists('./data/tokenized_{}merges.pt'.format(self.num_merges)):\n",
    "            print(\"Loading tokenized data from file\")\n",
    "            tokenized_text = torch.load('./data/tokenized_{}merges.pt'.format(self.num_merges),weights_only=False)\n",
    "            return tokenized_text\n",
    "\n",
    "        token_list = list(tokens.keys())\n",
    "        tokenized_text = []  # List to store the tokenized text\n",
    "        i = 0  # Index to keep track of the current position in the text\n",
    "\n",
    "        print(\"Nombre de tokens trouvés :\", len(token_list))\n",
    "\n",
    "        with tqdm.tqdm(total=len(self.text), position=0, leave=True) as pbar:\n",
    "            pbar.set_description(\"Tokenizing text\")\n",
    "            while i < len(self.text):\n",
    "                match = None\n",
    "                # loop through the tokens to find the longest match\n",
    "                for token in sorted(token_list, key=len, reverse=True):\n",
    "                    if self.text[i:i+len(token)] == token: \n",
    "                        match = token\n",
    "                        break\n",
    "                    \n",
    "                if match:  \n",
    "                    tokenized_text.append(match)\n",
    "                    i += len(match)  # Move the index to the end of the token\n",
    "                    pbar.update(len(match))\n",
    "                else:\n",
    "                    print(\"Token non trouvé pour le texte restant :\", self.text[i:])\n",
    "                    break \n",
    "\n",
    "        # Save the tokenized text\n",
    "        torch.save(tokenized_text, './data/tokenized_{}merges.pt'.format(self.num_merges))\n",
    "        return tokenized_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbac669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('./shakespeare.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29d0f0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized data from file\n",
      "Loading tokenized data from file\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(data, 1000)\n",
    "tokens, vocab = tokenizer.tokenize()\n",
    "tokenized_text = tokenizer.tokenize_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d5533a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f87c6e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', ' ', 'Citizen:', '\\n', 'Be', 'fore', ' ', 'we', ' ', 'pro', 'ce', 'ed', ' ', 'any', ' ', 'f', 'ur', 'ther', ',', ' ', 'hear', ' ', 'me', ' ', 'speak', '.', '\\n', '\\n', 'All', ':', '\\n', 'S', 'pe', 'ak', ',', ' ', 'speak', '.', '\\n', '\\n', 'First', ' ', 'Citizen:', '\\n', 'You', ' ', 'are', ' ', 'all', ' ', 'res', 'ol', 'ved', ' ', 'ra', 'ther', ' ', 'to', ' ', 'die', ' ', 'than', ' ', 'to', ' ', 'fa', 'mis', 'h', '?', '\\n', '\\n', 'All', ':', '\\n', 'R', 'es', 'ol', 'ved', '.', ' ', 'res', 'ol', 'ved', '.', '\\n', '\\n', 'First', ' ', 'Citizen:', '\\n', 'First', ',', ' ', 'you', ' ', 'know', ' ', 'C', 'a', 'i']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9ac26",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# CharDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ff1d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\" This class is used to create a PyTorch dataset from the tokenized text. \"\"\"\n",
    "    def __init__(self, tokenized_text, tokens, block_size = 32):\n",
    "\n",
    "        chars = list(tokens.keys())\n",
    "\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "        self.dataset = torch.tensor([self.stoi[ch] for ch in tokenized_text], dtype=torch.long)\n",
    "        self.set_data_mode()\n",
    "\n",
    "        \n",
    "        self.vocab_size = len(chars)\n",
    "        self.block_size = block_size\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return the number of blocks in the dataset, used by the DataLoader. \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Return the input and target sequence. \"\"\"\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        input_chunk = chunk[:-1]\n",
    "        target = chunk[1:]\n",
    "        return input_chunk, target\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\" Return the size of the vocabulary. \"\"\"\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_stoi(self):\n",
    "        return self.stoi\n",
    "\n",
    "    def get_itos(self):\n",
    "        return self.itos\n",
    "    \n",
    "    def set_data_mode(self, train_mode = True):\n",
    "        train_len = int(10/11 * len(self.dataset))\n",
    "\n",
    "        if train_mode:\n",
    "            self.data = self.dataset[:train_len]\n",
    "        else:\n",
    "            self.data = self.dataset[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "460f3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CharDataset(tokenized_text, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "effe5f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520780"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3d952fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52078"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_data_mode(train_mode = False)\n",
    "len(dataset.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
